{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a187c4b-625b-4ffe-93a7-18a7a1bd485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install unsloth\n",
    "# Get latest Unsloth\n",
    "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be107b-29f1-42d0-b71f-04f4e75fae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc386bb-76fe-480b-b78b-666f3840918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee612dd5-bfef-4064-b7e6-a480a02dbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32b47d-6c25-44b5-90c1-a18cd8bb6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "                               # EDIT HERE!\n",
    "    {\"from\": \"human\", \"value\": \"Classify the following movie review with sentiment analysis. Return 0 for negative and 1 for positive and nothing else.\\n\"+ds[\"train\"][0]['text']},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "output = model.generate(input_ids = inputs, max_new_tokens = 2, use_cache = True)\n",
    "print(int(tokenizer.decode(output[0,-2])))\n",
    "# _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)\n",
    "# tokenizer.batch_decode(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53300023-08a3-4b95-a30e-3aecb64e5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "Graph = Dict[str, Dict[str, Union[int, float]]]\n",
    "Sentence = List[str]\n",
    "\n",
    "def parse(content: str) -> List[Sentence]:\n",
    "    result = []\n",
    "\n",
    "    content = re.sub(r'<.*?>', ' ', content)\n",
    "    content = re.sub(r'[:;=8xX][-~]?[)(DPdOo/\\\\|*]+', ' ', content)\n",
    "    content = re.sub(r'[^\\w\\s.,!?]', ' ', content)\n",
    "    content = re.sub(r'\\s+', ' ', content).strip()\n",
    "    sentences = re.split(r'[.!?]', content)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(re.findall(r'\\b\\w+\\b', sentence)) == 0:\n",
    "            continue\n",
    "\n",
    "        tokens = ['<START>']\n",
    "        words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        tokens.extend(words)\n",
    "        tokens.append('<END>')\n",
    "        result.append(tokens)\n",
    "\n",
    "    return result\n",
    "\n",
    "def encode(sentences: List[Sentence]) -> Tuple[Graph, Dict]:\n",
    "    graph = {}\n",
    "    for sentence in sentences:\n",
    "        prev = sentence[0]\n",
    "        for current, next_ in zip(sentence[1:-1], sentence[2:]):\n",
    "            if prev not in graph:\n",
    "                graph[prev] = {}\n",
    "            graph[prev][current] = graph[prev].get(current, 0) + 1\n",
    "            prev = current\n",
    "        if prev not in graph:\n",
    "            graph[prev] = {}\n",
    "        graph[prev][sentence[-1]] = graph[prev].get(sentence[-1], 0) + 1\n",
    "    return graph, {}\n",
    "\n",
    "def weight_graph(graph: Graph, sentences: List[Sentence]) -> Graph:\n",
    "    weighted_graph = {}\n",
    "    all_tokens = [' '.join(sentence) for sentence in sentences]\n",
    "\n",
    "    if not all_tokens:\n",
    "        print(\"Warning: No valid tokens for TF-IDF calculation.\")\n",
    "        return weighted_graph\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_tokens)\n",
    "    tfidf_scores = {word: tfidf for word, tfidf in zip(vectorizer.get_feature_names_out(), tfidf_matrix.max(axis=0).toarray()[0])}\n",
    "\n",
    "    for tail, heads in graph.items():\n",
    "        total = sum(heads.values())\n",
    "        weighted_graph[tail] = {}\n",
    "        for head, count in heads.items():\n",
    "            pos = pos_tag([head])[0][1]\n",
    "            if pos in ['JJ', 'RB']: \n",
    "                tfidf_weight = tfidf_scores.get(head, 1.0) * 3.0 \n",
    "            elif pos in ['NN', 'VB']: \n",
    "                tfidf_weight = tfidf_scores.get(head, 1.0) * 2.0\n",
    "            else:\n",
    "                tfidf_weight = tfidf_scores.get(head, 1.0)\n",
    "\n",
    "            weighted_graph[tail][head] = (1 - (count / total)) * tfidf_weight\n",
    "    return weighted_graph\n",
    "\n",
    "def traverse(graph: Graph, max_len: int = 5) -> List[Sentence]:\n",
    "    paths = []\n",
    "    fringe = [(['<START>'], 0)]\n",
    "    while fringe and len(paths) < max_len:\n",
    "        path, cost = fringe.pop(0)\n",
    "        tail = path[-1]\n",
    "        if tail == '<END>' and len(path) > 3:\n",
    "            paths.append((path, cost / len(path)))\n",
    "            continue\n",
    "        for head, weight in graph.get(tail, {}).items():\n",
    "            if head not in path:\n",
    "                fringe.append((path + [head], cost + weight))\n",
    "    paths.sort(key=lambda x: x[1])\n",
    "    return [p[0] for p in paths if p[0][-1] == '<END>']\n",
    "\n",
    "def compress_content(content: str):\n",
    "    sentences = re.split(r'[.!?]', content.strip())\n",
    "    compressed = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) < 3:\n",
    "            continue\n",
    "        tokens = parse(sentence)\n",
    "        graph, _ = encode(tokens)\n",
    "        weighted_graph = weight_graph(graph, tokens)\n",
    "        paths = traverse(weighted_graph)\n",
    "        compressed.extend([' '.join(path[1:-1]) for path in paths])\n",
    "\n",
    "    compressed_text = '\\n'.join(list(set(compressed)))\n",
    "    return compressed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dfdb9-45d4-4d3a-a848-1e9197f5f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=12\n",
    "print(i, ds[\"train\"][i]['text'])\n",
    "print(i, compress_content(ds[\"train\"][i]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83351a2-edd9-4d38-a791-58fe00b88d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "# for i in range(len(ds[\"train\"])):\n",
    "for i in range(1000):\n",
    "  messages = [\n",
    "      {\"from\": \"human\", \"value\": \"Classify the following movie review with sentiment analysis. Return 0 for negative and 1 for positive and nothing else.\"+ds[\"train\"][i]['text']},\n",
    "  ]\n",
    "  inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "  text_streamer = TextStreamer(tokenizer)\n",
    "  output = model.generate(input_ids = inputs, max_new_tokens = 2, use_cache = True)\n",
    "  if(int(tokenizer.decode(output[0,-2])) == ds[\"train\"][i]['label']):\n",
    "    correct += 1\n",
    "  if((i+1) % 100 == 0):\n",
    "    print(correct,'correct in',i+1)\n",
    "print(correct/len(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328e771-7141-4c13-8539-e6e166beb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee7625-a24e-4f3e-8d90-715e30b0d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "Graph = Dict[str, Dict[str, Union[int, float]]]\n",
    "Sentence = List[str]\n",
    "\n",
    "def parse(content: str) -> List[Sentence]:\n",
    "    result = []\n",
    "    content = re.sub(r'<.*?>', ' ', content)\n",
    "    content = re.sub(r'[:;=8xX][-~]?[)(DPdOo/\\\\|*]+', ' ', content)\n",
    "    content = re.sub(r'[^\\w\\s.,!?]', ' ', content)\n",
    "    content = re.sub(r'\\s+', ' ', content).strip()\n",
    "    sentences = re.split(r'[.!?]', content)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(re.findall(r'\\b\\w+\\b', sentence)) == 0:\n",
    "            continue\n",
    "\n",
    "        tokens = ['<START>']\n",
    "        words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        tokens.extend(words)\n",
    "        tokens.append('<END>')\n",
    "        result.append(tokens)\n",
    "\n",
    "    return result\n",
    "\n",
    "def encode(sentences: List[Sentence]) -> Tuple[Graph, Dict]:\n",
    "    graph = {}\n",
    "    for sentence in sentences:\n",
    "        prev = sentence[0]\n",
    "        for current, next_ in zip(sentence[1:-1], sentence[2:]):\n",
    "            if prev not in graph:\n",
    "                graph[prev] = {}\n",
    "            graph[prev][current] = graph[prev].get(current, 0) + 1\n",
    "            prev = current\n",
    "        if prev not in graph:\n",
    "            graph[prev] = {}\n",
    "        graph[prev][sentence[-1]] = graph[prev].get(sentence[-1], 0) + 1\n",
    "    return graph, {}\n",
    "\n",
    "def weight_graph(graph: Graph, sentences: List[Sentence]) -> Graph:\n",
    "    weighted_graph = {}\n",
    "    all_tokens = [' '.join(sentence) for sentence in sentences]\n",
    "\n",
    "    if not all_tokens:\n",
    "        print(\"Warning: No valid tokens for TF-IDF calculation.\")\n",
    "        return weighted_graph\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_tokens)\n",
    "    tfidf_scores = {word: tfidf for word, tfidf in zip(vectorizer.get_feature_names_out(), tfidf_matrix.max(axis=0).toarray()[0])}\n",
    "\n",
    "    for tail, heads in graph.items():\n",
    "        total = sum(heads.values())\n",
    "        weighted_graph[tail] = {}\n",
    "        for head, count in heads.items():\n",
    "            pos = pos_tag([head])[0][1]\n",
    "            if pos in ['JJ', 'RB']:\n",
    "                tfidf_weight = tfidf_scores.get(head, 1.0) * 3.0\n",
    "            elif pos in ['NN', 'VB']:\n",
    "                tfidf_weight = tfidf_scores.get(head, 1.0) * 2.0\n",
    "            else:\n",
    "                tfidf_weight = tfidf_scores.get(head, 1.0)\n",
    "\n",
    "            weighted_graph[tail][head] = (1 - (count / total)) * tfidf_weight\n",
    "    return weighted_graph\n",
    "\n",
    "def remove_duplicates(sentences: List[str]) -> List[str]:\n",
    "    unique_sentences = []\n",
    "    seen = set()\n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen:\n",
    "            seen.add(sentence)\n",
    "            unique_sentences.append(sentence)\n",
    "    return unique_sentences\n",
    "\n",
    "def filter_sentences_by_similarity(sentences: List[str], threshold: float = 0.7) -> List[str]:\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    if tfidf_matrix.shape[1] == 0:\n",
    "        print(\"Warning: Empty vocabulary; returning original sentences.\")\n",
    "        return sentences\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "    selected_sentences = []\n",
    "    selected_indices = set()\n",
    "    for i in range(len(sentences)):\n",
    "        if i in selected_indices:\n",
    "            continue\n",
    "        selected_sentences.append(sentences[i])\n",
    "        selected_indices.add(i)\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            if similarity_matrix[i, j] > threshold:\n",
    "                selected_indices.add(j)\n",
    "    return selected_sentences\n",
    "\n",
    "def traverse(graph: Graph, max_len: int = 5) -> List[Sentence]:\n",
    "    paths = []\n",
    "    fringe = [(['<START>'], 0)]\n",
    "    while fringe and len(paths) < max_len:\n",
    "        path, cost = fringe.pop(0)\n",
    "        tail = path[-1]\n",
    "        if tail == '<END>' and len(path) > 3:\n",
    "            paths.append((path, cost / len(path)))\n",
    "            continue\n",
    "        for head, weight in graph.get(tail, {}).items():\n",
    "            if head not in path:\n",
    "                fringe.append((path + [head], cost + weight))\n",
    "    paths.sort(key=lambda x: x[1])\n",
    "    return [p[0] for p in paths if p[0][-1] == '<END>']\n",
    "\n",
    "def process_file(input_path: str, output_path: str):\n",
    "    print(f\"Processing file: {input_path}\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(compressed_text)\n",
    "\n",
    "def compress_content(content: str):\n",
    "    sentences = re.split(r'[.!?]', content.strip())\n",
    "    compressed = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) < 3:\n",
    "            continue\n",
    "        tokens = parse(sentence)\n",
    "        graph, _ = encode(tokens)\n",
    "        weighted_graph = weight_graph(graph, tokens)\n",
    "        paths = traverse(weighted_graph)\n",
    "        compressed.extend([' '.join(path[1:-1]) for path in paths])\n",
    "\n",
    "    compressed = remove_duplicates(compressed)\n",
    "    compressed = filter_sentences_by_similarity(compressed)\n",
    "    compressed_text = '\\n'.join(compressed)\n",
    "    return compressed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fb4c8-2c1c-496e-a34e-c96fc3c8946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = 0\n",
    "correct = 0\n",
    "unknown = 0\n",
    "correct_ls = []\n",
    "# for i in range(len(ds[\"train\"])):\n",
    "for i in range(1000):\n",
    "  messages = [\n",
    "                                # EDIT HERE!\n",
    "      {\"from\": \"human\", \"value\": \"Classify the following movie review with sentiment analyasis. Return 0 for negative and 1 for positive and nothing else.\\n\\n\"+compress_content(ds[\"train\"][i]['text'])},\n",
    "  ]\n",
    "  inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "  text_streamer = TextStreamer(tokenizer)\n",
    "  output = model.generate(input_ids = inputs, max_new_tokens = 2, use_cache = True)\n",
    "  if tokenizer.decode(output[0,-2]) != '0' and tokenizer.decode(output[0,-2]) != '1':\n",
    "    # print(i, compress_content(ds[\"train\"][i]['text']))\n",
    "    # print(i, ds[\"train\"][i]['text'])\n",
    "    unknown += 1\n",
    "  elif int(tokenizer.decode(output[0,-2])) == ds[\"train\"][i]['label']:\n",
    "    correct += 1\n",
    "    correct_ls.append(i)\n",
    "  else:\n",
    "    incorrect += 1\n",
    "  if((i+1) % 100 == 0):\n",
    "    print(f\"Total: {i+1}. Correct: {correct}. Incorrect: {incorrect}. Unknown: {unknown}\")\n",
    "print(correct_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0f436-323d-4dfc-a47f-e6393dc669f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=13\n",
    "print(i, len(ds[\"train\"][i]['text']), ds[\"train\"][i]['text'])\n",
    "print(i, len(compress_content(ds[\"train\"][i]['text'])), compress_content(ds[\"train\"][i]['text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baddiff",
   "language": "python",
   "name": "baddiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
